{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4201e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKL_DATASET_DIR: /home/connor/Documents/Development/Gesture-Recognition/Dataset Work/pkl_dataset\n",
      "Created subject subdirs: subj01, subj02, subj03, subj04, subj05, subj06, subj07, subj08, subj09, subj10\n"
     ]
    }
   ],
   "source": [
    "# MAKE THE DIRECTORIES FOR SAVING THE PKL DATASET IF THEY DO NOT EXIST\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "PKL_DATASET_DIR = WORKSPACE / \"pkl_dataset\"\n",
    "SUBJECTS = [f\"subj{n:02d}\" for n in range(1, 11)]\n",
    "\n",
    "PKL_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for subj in SUBJECTS:\n",
    "    (PKL_DATASET_DIR / subj).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PKL_DATASET_DIR:\", PKL_DATASET_DIR)\n",
    "print(\"Created subject subdirs:\", \", \".join(SUBJECTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0d7416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_ROOT: /home/connor/Documents/Development/Gesture-Recognition/Dataset Work/osfstorage-archive\n",
      "DST_ROOT: /home/connor/Documents/Development/Gesture-Recognition/Dataset Work/pkl_dataset\n",
      "MAX_WORKERS: 12\n",
      "VALIDATE: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "SRC_ROOT = WORKSPACE / 'osfstorage-archive'\n",
    "DST_ROOT = WORKSPACE / 'pkl_dataset'\n",
    "\n",
    "SUBJECTS = [f'subj{n:02d}' for n in range(1, 11)]\n",
    "\n",
    "# Concurrency: tune if you want (I/O + CPU mix).\n",
    "MAX_WORKERS = min(12, (os.cpu_count() or 4))\n",
    "\n",
    "# Validation is an extra load of the pickle (but avoids comparing full arrays).\n",
    "VALIDATE = True\n",
    "\n",
    "print('SRC_ROOT:', SRC_ROOT)\n",
    "print('DST_ROOT:', DST_ROOT)\n",
    "print('MAX_WORKERS:', MAX_WORKERS)\n",
    "print('VALIDATE:', VALIDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb44c2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered .mat files: 0\n",
      "Example: None\n"
     ]
    }
   ],
   "source": [
    "def discover_mat_files() -> list[Path]:\n",
    "    mats: list[Path] = []\n",
    "    for subj in SUBJECTS:\n",
    "        subj_dir = SRC_ROOT / subj\n",
    "        if not subj_dir.exists():\n",
    "            raise FileNotFoundError(f'Missing subject directory: {subj_dir}')\n",
    "        mats.extend(sorted(subj_dir.rglob('*.mat')))\n",
    "    return mats\n",
    "\n",
    "mat_files = discover_mat_files()\n",
    "print('Discovered .mat files:', len(mat_files))\n",
    "print('Example:', mat_files[0] if mat_files else 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f20054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ConvertResult:\n",
    "    mat_path: Path\n",
    "    pkl_path: Path\n",
    "    ok: bool\n",
    "    seconds: float\n",
    "    error: str | None = None\n",
    "\n",
    "def dst_path_for(mat_path: Path) -> Path:\n",
    "    # Expect mat_path like .../osfstorage-archive/subjXX/<optional subdirs>/file.mat\n",
    "    # Mirror any nested subfolders under pkl_dataset/subjXX/... and keep filename.\n",
    "    parts = mat_path.parts\n",
    "    try:\n",
    "        i = parts.index('osfstorage-archive')\n",
    "    except ValueError:\n",
    "        raise ValueError(f'Path is not under osfstorage-archive: {mat_path}')\n",
    "\n",
    "    subj = parts[i + 1]\n",
    "    if subj not in SUBJECTS:\n",
    "        raise ValueError(f'Not a supported subject folder: {subj} (path: {mat_path})')\n",
    "\n",
    "    rel_under_subj = Path(*parts[i + 2 : -1])  # may be '.'\n",
    "    out_dir = DST_ROOT / subj / rel_under_subj\n",
    "    return out_dir / (mat_path.stem + '.pkl')\n",
    "\n",
    "def lightweight_validate(mat_dict: dict, pkl_dict: dict, mat_path: Path) -> None:\n",
    "    # Keys\n",
    "    mat_keys = sorted(mat_dict.keys())\n",
    "    pkl_keys = sorted(pkl_dict.keys())\n",
    "    if mat_keys != pkl_keys:\n",
    "        raise ValueError(f'Key mismatch for {mat_path}: MAT has {len(mat_keys)} keys, PKL has {len(pkl_keys)} keys')\n",
    "\n",
    "    # Types + basic ndarray structure\n",
    "    for k in mat_keys:\n",
    "        a = mat_dict[k]\n",
    "        b = pkl_dict[k]\n",
    "        if type(a) is not type(b):\n",
    "            raise TypeError(f'Type mismatch for {mat_path} key={k}: MAT={type(a)} PKL={type(b)}')\n",
    "        if isinstance(a, np.ndarray):\n",
    "            if a.shape != b.shape or a.dtype != b.dtype:\n",
    "                raise ValueError(\n",
    "                    f'Array mismatch for {mat_path} key={k}: ' \n",
    "                    f'MAT shape/dtype={a.shape}/{a.dtype} vs PKL={b.shape}/{b.dtype}'\n",
    "                )\n",
    "\n",
    "def convert_one(mat_path: Path) -> ConvertResult:\n",
    "    t0 = time.perf_counter()\n",
    "    pkl_path = dst_path_for(mat_path)\n",
    "    try:\n",
    "        pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load ENTIRE .mat file dict\n",
    "        mat_dict = loadmat(mat_path)\n",
    "\n",
    "        # Write pickle\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(mat_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Lightweight validation (reload pickle, compare keys/types/shape/dtype)\n",
    "        if VALIDATE:\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                pkl_dict = pickle.load(f)\n",
    "            lightweight_validate(mat_dict, pkl_dict, mat_path)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=True, seconds=t1 - t0)\n",
    "    except Exception as e:\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=False, seconds=t1 - t0, error=str(e))\n",
    "\n",
    "# Run conversion with multi-threading\n",
    "results: list[ConvertResult] = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(convert_one, mp): mp for mp in mat_files}\n",
    "    for idx, fut in enumerate(as_completed(futures), start=1):\n",
    "        res = fut.result()\n",
    "        results.append(res)\n",
    "        if idx % 25 == 0 or idx == len(futures):\n",
    "            ok = sum(r.ok for r in results)\n",
    "            print(f'Progress: {idx}/{len(futures)} | ok={ok} | failed={idx-ok}')\n",
    "\n",
    "end = time.perf_counter()\n",
    "print('Done in %.2f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc4a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0\n",
      "Succeeded: 0\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "total = len(results)\n",
    "ok = sum(r.ok for r in results)\n",
    "failed = total - ok\n",
    "\n",
    "print('Total:', total)\n",
    "print('Succeeded:', ok)\n",
    "print('Failed:', failed)\n",
    "\n",
    "if failed:\n",
    "    print('--- Failures (up to 25) ---')\n",
    "    for r in [x for x in results if not x.ok][:25]:\n",
    "        print('MAT:', r.mat_path)\n",
    "        print('PKL:', r.pkl_path)\n",
    "        print('ERR:', r.error)\n",
    "        print('---')\n",
    "\n",
    "# Basic timing summary (conversion time per file, includes optional validation)\n",
    "times = np.array([r.seconds for r in results], dtype=float) if results else np.array([], dtype=float)\n",
    "if times.size:\n",
    "    print('Mean seconds/file:', float(times.mean()))\n",
    "    print('Median seconds/file:', float(np.median(times)))\n",
    "    print('p95 seconds/file:', float(np.quantile(times, 0.95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d13e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/envs/gesture-recognition-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Resampling PKLs: 100%|██████████| 2340/2340 [01:13<00:00, 31.80file/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample, decimate\n",
    "import os\n",
    "from tqdm.auto import tqdm  # <-- progress bar\n",
    "\n",
    "## Down sample the EMG data and interpolate the glove data\n",
    "## The EMG data is 2048Hz nxt where n=number of channels (134) and t is number of time points\n",
    "## The glove data is 256Hz mxt where m=number of joint angles (18) and t is number of time points\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "PKL_ROOT = ROOT / 'pkl_dataset'\n",
    "OUT_ROOT = ROOT / 'pkl_dataset_resampled'  # new output folder (non-destructive)\n",
    "\n",
    "#### 1: check data in one file to confirm data shape and available vars\n",
    "dirs = [entry.name for entry in os.scandir(PKL_ROOT) if entry.is_dir()]\n",
    "\n",
    "# Build a flat list of work items so tqdm can show an accurate total\n",
    "work = []\n",
    "for dir in dirs:\n",
    "    files = [x for x in os.listdir(PKL_ROOT / Path(dir)) if '.pkl' in x]\n",
    "    for file in files:\n",
    "        if 'calibration' in file:\n",
    "            continue\n",
    "        work.append((dir, file))\n",
    "\n",
    "for dir, file in tqdm(work, desc=\"Resampling PKLs\", unit=\"file\"):\n",
    "    df = pd.read_pickle(PKL_ROOT / Path(dir) / Path(file))\n",
    "\n",
    "    # upsample glove data\n",
    "    glove_fs = 256\n",
    "    resampled_glove_fs = 512\n",
    "    t = np.linspace(0, df['glove'].shape[-1]/glove_fs, df['glove'].shape[-1], endpoint=True)\n",
    "    upsampled_glove_data = resample(\n",
    "        df['glove'],\n",
    "        num=int(resampled_glove_fs/glove_fs) * df['glove'].shape[-1],\n",
    "        t=t,\n",
    "        axis=1\n",
    "    )[0]\n",
    "\n",
    "    # downsample emg data\n",
    "    emg_fs = 2048\n",
    "    downsampled_emg_fs = 512\n",
    "    downsampled_emg_data = decimate(df['emg'], q=int(emg_fs/downsampled_emg_fs), n=2, axis=1)\n",
    "\n",
    "    df['emg'] = downsampled_emg_data\n",
    "    df['glove'] = upsampled_glove_data\n",
    "\n",
    "    out_dir = OUT_ROOT / dir        # join path with string\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_file = out_dir / file\n",
    "\n",
    "    with out_file.open(\"wb\") as f:\n",
    "        pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture-recognition-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
