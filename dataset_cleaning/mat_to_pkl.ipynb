{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858d9175",
   "metadata": {},
   "source": [
    "# .mat to .pkl file conversion\n",
    "This project converts the SEEDS dataset to pkl format for faster python processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample, decimate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e43c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE THE DIRECTORIES FOR SAVING THE PKL DATASET IF THEY DO NOT EXIST\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "PKL_DATASET_DIR = WORKSPACE / \"pkl_dataset\"\n",
    "SUBJECTS = [f\"subj{n:02d}\" for n in range(1, 11)]\n",
    "\n",
    "PKL_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for subj in SUBJECTS:\n",
    "    (PKL_DATASET_DIR / subj).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PKL_DATASET_DIR:\", PKL_DATASET_DIR)\n",
    "print(\"Created subject subdirs:\", \", \".join(SUBJECTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "SRC_ROOT = WORKSPACE / 'osfstorage-archive'\n",
    "DST_ROOT = WORKSPACE / 'pkl_dataset'\n",
    "\n",
    "SUBJECTS = [f'subj{n:02d}' for n in range(1, 11)]\n",
    "MAX_WORKERS = min(12, (os.cpu_count() or 4))\n",
    "\n",
    "VALIDATE = True # disable to speed up processing if taking way too long\n",
    "\n",
    "print('SRC_ROOT:', SRC_ROOT)\n",
    "print('DST_ROOT:', DST_ROOT)\n",
    "print('MAX_WORKERS:', MAX_WORKERS)\n",
    "print('VALIDATE:', VALIDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_mat_files() -> list[Path]:\n",
    "    mats: list[Path] = []\n",
    "    for subj in SUBJECTS:\n",
    "        subj_dir = SRC_ROOT / subj\n",
    "        if not subj_dir.exists():\n",
    "            raise FileNotFoundError(f'Missing subject directory: {subj_dir}')\n",
    "        mats.extend(sorted(subj_dir.rglob('*.mat')))\n",
    "    return mats\n",
    "\n",
    "mat_files = discover_mat_files()\n",
    "print('Discovered .mat files:', len(mat_files))\n",
    "print('Example:', mat_files[0] if mat_files else 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df64cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConvertResult:\n",
    "    mat_path: Path\n",
    "    pkl_path: Path\n",
    "    ok: bool\n",
    "    seconds: float\n",
    "    error: str | None = None\n",
    "\n",
    "def dst_path_for(mat_path: Path) -> Path:\n",
    "    parts = mat_path.parts\n",
    "    try:\n",
    "        i = parts.index('osfstorage-archive')\n",
    "    except ValueError:\n",
    "        raise ValueError(f'Path is not under osfstorage-archive: {mat_path}')\n",
    "\n",
    "    subj = parts[i + 1]\n",
    "    if subj not in SUBJECTS:\n",
    "        raise ValueError(f'Not a supported subject folder: {subj} (path: {mat_path})')\n",
    "\n",
    "    rel_under_subj = Path(*parts[i + 2 : -1])  # may be '.'\n",
    "    out_dir = DST_ROOT / subj / rel_under_subj\n",
    "    return out_dir / (mat_path.stem + '.pkl')\n",
    "\n",
    "def lightweight_validate(mat_dict: dict, pkl_dict: dict, mat_path: Path) -> None:\n",
    "    mat_keys = sorted(mat_dict.keys())\n",
    "    pkl_keys = sorted(pkl_dict.keys())\n",
    "    if mat_keys != pkl_keys:\n",
    "        raise ValueError(f'Key mismatch for {mat_path}: MAT has {len(mat_keys)} keys, PKL has {len(pkl_keys)} keys')\n",
    "\n",
    "    for k in mat_keys:\n",
    "        a = mat_dict[k]\n",
    "        b = pkl_dict[k]\n",
    "        if type(a) is not type(b):\n",
    "            raise TypeError(f'Type mismatch for {mat_path} key={k}: MAT={type(a)} PKL={type(b)}')\n",
    "        if isinstance(a, np.ndarray):\n",
    "            if a.shape != b.shape or a.dtype != b.dtype:\n",
    "                raise ValueError(\n",
    "                    f'Array mismatch for {mat_path} key={k}: ' \n",
    "                    f'MAT shape/dtype={a.shape}/{a.dtype} vs PKL={b.shape}/{b.dtype}'\n",
    "                )\n",
    "\n",
    "def convert_one(mat_path: Path) -> ConvertResult:\n",
    "    t0 = time.perf_counter()\n",
    "    pkl_path = dst_path_for(mat_path)\n",
    "    try:\n",
    "        pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load ENTIRE .mat file dict\n",
    "        mat_dict = loadmat(mat_path)\n",
    "\n",
    "        # Write pickle\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(mat_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Lightweight validation (reload pickle, compare keys/types/shape/dtype)\n",
    "        if VALIDATE:\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                pkl_dict = pickle.load(f)\n",
    "            lightweight_validate(mat_dict, pkl_dict, mat_path)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=True, seconds=t1 - t0)\n",
    "    except Exception as e:\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=False, seconds=t1 - t0, error=str(e))\n",
    "\n",
    "# Run conversion with multi-threading\n",
    "results: list[ConvertResult] = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(convert_one, mp): mp for mp in mat_files}\n",
    "    for idx, fut in enumerate(as_completed(futures), start=1):\n",
    "        res = fut.result()\n",
    "        results.append(res)\n",
    "        if idx % 25 == 0 or idx == len(futures):\n",
    "            ok = sum(r.ok for r in results)\n",
    "            print(f'Progress: {idx}/{len(futures)} | ok={ok} | failed={idx-ok}')\n",
    "\n",
    "end = time.perf_counter()\n",
    "print('Done in %.2f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(results)\n",
    "ok = sum(r.ok for r in results)\n",
    "failed = total - ok\n",
    "\n",
    "print('Total:', total)\n",
    "print('Succeeded:', ok)\n",
    "print('Failed:', failed)\n",
    "\n",
    "if failed:\n",
    "    print('--- Failures (up to 25) ---')\n",
    "    for r in [x for x in results if not x.ok][:25]:\n",
    "        print('MAT:', r.mat_path)\n",
    "        print('PKL:', r.pkl_path)\n",
    "        print('ERR:', r.error)\n",
    "        print('---')\n",
    "\n",
    "# timing summary (not working at the moment)\n",
    "times = np.array([r.seconds for r in results], dtype=float) if results else np.array([], dtype=float)\n",
    "if times.size:\n",
    "    print('Mean seconds/file:', float(times.mean()))\n",
    "    print('Median seconds/file:', float(np.median(times)))\n",
    "    print('p95 seconds/file:', float(np.quantile(times, 0.95)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
