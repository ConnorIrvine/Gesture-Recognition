{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4201e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKL_DATASET_DIR: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/pkl_dataset\n",
      "Created subject subdirs: subj01, subj02, subj03, subj04, subj05, subj06, subj07, subj08, subj09, subj10\n"
     ]
    }
   ],
   "source": [
    "# MAKE THE DIRECTORIES FOR SAVING THE PKL DATASET IF THEY DO NOT EXIST\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "PKL_DATASET_DIR = WORKSPACE / \"pkl_dataset\"\n",
    "SUBJECTS = [f\"subj{n:02d}\" for n in range(1, 11)]\n",
    "\n",
    "PKL_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for subj in SUBJECTS:\n",
    "    (PKL_DATASET_DIR / subj).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PKL_DATASET_DIR:\", PKL_DATASET_DIR)\n",
    "print(\"Created subject subdirs:\", \", \".join(SUBJECTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0d7416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_ROOT: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/osfstorage-archive\n",
      "DST_ROOT: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/pkl_dataset\n",
      "MAX_WORKERS: 8\n",
      "VALIDATE: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "SRC_ROOT = WORKSPACE / 'osfstorage-archive'\n",
    "DST_ROOT = WORKSPACE / 'pkl_dataset'\n",
    "\n",
    "SUBJECTS = [f'subj{n:02d}' for n in range(1, 11)]\n",
    "\n",
    "# Concurrency: tune if you want (I/O + CPU mix).\n",
    "MAX_WORKERS = min(12, (os.cpu_count() or 4))\n",
    "\n",
    "# Validation is an extra load of the pickle (but avoids comparing full arrays).\n",
    "VALIDATE = True\n",
    "\n",
    "print('SRC_ROOT:', SRC_ROOT)\n",
    "print('DST_ROOT:', DST_ROOT)\n",
    "print('MAX_WORKERS:', MAX_WORKERS)\n",
    "print('VALIDATE:', VALIDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb44c2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered .mat files: 2350\n",
      "Example: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/osfstorage-archive/subj01/detop_exp01_subj01_Sess1_01_01.mat\n"
     ]
    }
   ],
   "source": [
    "def discover_mat_files() -> list[Path]:\n",
    "    mats: list[Path] = []\n",
    "    for subj in SUBJECTS:\n",
    "        subj_dir = SRC_ROOT / subj\n",
    "        if not subj_dir.exists():\n",
    "            raise FileNotFoundError(f'Missing subject directory: {subj_dir}')\n",
    "        mats.extend(sorted(subj_dir.rglob('*.mat')))\n",
    "    return mats\n",
    "\n",
    "mat_files = discover_mat_files()\n",
    "print('Discovered .mat files:', len(mat_files))\n",
    "print('Example:', mat_files[0] if mat_files else 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f20054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 25/2350 | ok=25 | failed=0\n",
      "Progress: 50/2350 | ok=50 | failed=0\n",
      "Progress: 75/2350 | ok=75 | failed=0\n",
      "Progress: 100/2350 | ok=100 | failed=0\n",
      "Progress: 125/2350 | ok=125 | failed=0\n",
      "Progress: 150/2350 | ok=150 | failed=0\n",
      "Progress: 175/2350 | ok=175 | failed=0\n",
      "Progress: 200/2350 | ok=200 | failed=0\n",
      "Progress: 225/2350 | ok=225 | failed=0\n",
      "Progress: 250/2350 | ok=250 | failed=0\n",
      "Progress: 275/2350 | ok=275 | failed=0\n",
      "Progress: 300/2350 | ok=300 | failed=0\n",
      "Progress: 325/2350 | ok=325 | failed=0\n",
      "Progress: 350/2350 | ok=350 | failed=0\n",
      "Progress: 375/2350 | ok=375 | failed=0\n",
      "Progress: 400/2350 | ok=400 | failed=0\n",
      "Progress: 425/2350 | ok=425 | failed=0\n",
      "Progress: 450/2350 | ok=450 | failed=0\n",
      "Progress: 475/2350 | ok=475 | failed=0\n",
      "Progress: 500/2350 | ok=500 | failed=0\n",
      "Progress: 525/2350 | ok=525 | failed=0\n",
      "Progress: 550/2350 | ok=550 | failed=0\n",
      "Progress: 575/2350 | ok=575 | failed=0\n",
      "Progress: 600/2350 | ok=600 | failed=0\n",
      "Progress: 625/2350 | ok=625 | failed=0\n",
      "Progress: 650/2350 | ok=650 | failed=0\n",
      "Progress: 675/2350 | ok=675 | failed=0\n",
      "Progress: 700/2350 | ok=700 | failed=0\n",
      "Progress: 725/2350 | ok=725 | failed=0\n",
      "Progress: 750/2350 | ok=750 | failed=0\n",
      "Progress: 775/2350 | ok=775 | failed=0\n",
      "Progress: 800/2350 | ok=800 | failed=0\n",
      "Progress: 825/2350 | ok=825 | failed=0\n",
      "Progress: 850/2350 | ok=850 | failed=0\n",
      "Progress: 875/2350 | ok=875 | failed=0\n",
      "Progress: 900/2350 | ok=900 | failed=0\n",
      "Progress: 925/2350 | ok=925 | failed=0\n",
      "Progress: 950/2350 | ok=950 | failed=0\n",
      "Progress: 975/2350 | ok=975 | failed=0\n",
      "Progress: 1000/2350 | ok=1000 | failed=0\n",
      "Progress: 1025/2350 | ok=1025 | failed=0\n",
      "Progress: 1050/2350 | ok=1050 | failed=0\n",
      "Progress: 1075/2350 | ok=1075 | failed=0\n",
      "Progress: 1100/2350 | ok=1100 | failed=0\n",
      "Progress: 1125/2350 | ok=1125 | failed=0\n",
      "Progress: 1150/2350 | ok=1150 | failed=0\n",
      "Progress: 1175/2350 | ok=1175 | failed=0\n",
      "Progress: 1200/2350 | ok=1200 | failed=0\n",
      "Progress: 1225/2350 | ok=1225 | failed=0\n",
      "Progress: 1250/2350 | ok=1250 | failed=0\n",
      "Progress: 1275/2350 | ok=1275 | failed=0\n",
      "Progress: 1300/2350 | ok=1300 | failed=0\n",
      "Progress: 1325/2350 | ok=1325 | failed=0\n",
      "Progress: 1350/2350 | ok=1350 | failed=0\n",
      "Progress: 1375/2350 | ok=1375 | failed=0\n",
      "Progress: 1400/2350 | ok=1400 | failed=0\n",
      "Progress: 1425/2350 | ok=1425 | failed=0\n",
      "Progress: 1450/2350 | ok=1450 | failed=0\n",
      "Progress: 1475/2350 | ok=1475 | failed=0\n",
      "Progress: 1500/2350 | ok=1500 | failed=0\n",
      "Progress: 1525/2350 | ok=1525 | failed=0\n",
      "Progress: 1550/2350 | ok=1550 | failed=0\n",
      "Progress: 1575/2350 | ok=1575 | failed=0\n",
      "Progress: 1600/2350 | ok=1600 | failed=0\n",
      "Progress: 1625/2350 | ok=1625 | failed=0\n",
      "Progress: 1650/2350 | ok=1650 | failed=0\n",
      "Progress: 1675/2350 | ok=1675 | failed=0\n",
      "Progress: 1700/2350 | ok=1700 | failed=0\n",
      "Progress: 1725/2350 | ok=1725 | failed=0\n",
      "Progress: 1750/2350 | ok=1750 | failed=0\n",
      "Progress: 1775/2350 | ok=1775 | failed=0\n",
      "Progress: 1800/2350 | ok=1800 | failed=0\n",
      "Progress: 1825/2350 | ok=1825 | failed=0\n",
      "Progress: 1850/2350 | ok=1850 | failed=0\n",
      "Progress: 1875/2350 | ok=1875 | failed=0\n",
      "Progress: 1900/2350 | ok=1900 | failed=0\n",
      "Progress: 1925/2350 | ok=1925 | failed=0\n",
      "Progress: 1950/2350 | ok=1950 | failed=0\n",
      "Progress: 1975/2350 | ok=1975 | failed=0\n",
      "Progress: 2000/2350 | ok=2000 | failed=0\n",
      "Progress: 2025/2350 | ok=2025 | failed=0\n",
      "Progress: 2050/2350 | ok=2050 | failed=0\n",
      "Progress: 2075/2350 | ok=2075 | failed=0\n",
      "Progress: 2100/2350 | ok=2100 | failed=0\n",
      "Progress: 2125/2350 | ok=2125 | failed=0\n",
      "Progress: 2150/2350 | ok=2150 | failed=0\n",
      "Progress: 2175/2350 | ok=2175 | failed=0\n",
      "Progress: 2200/2350 | ok=2200 | failed=0\n",
      "Progress: 2225/2350 | ok=2225 | failed=0\n",
      "Progress: 2250/2350 | ok=2250 | failed=0\n",
      "Progress: 2275/2350 | ok=2275 | failed=0\n",
      "Progress: 2300/2350 | ok=2300 | failed=0\n",
      "Progress: 2325/2350 | ok=2325 | failed=0\n",
      "Progress: 2350/2350 | ok=2350 | failed=0\n",
      "Done in 39.45 seconds\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ConvertResult:\n",
    "    mat_path: Path\n",
    "    pkl_path: Path\n",
    "    ok: bool\n",
    "    seconds: float\n",
    "    error: str | None = None\n",
    "\n",
    "def dst_path_for(mat_path: Path) -> Path:\n",
    "    # Expect mat_path like .../osfstorage-archive/subjXX/<optional subdirs>/file.mat\n",
    "    # Mirror any nested subfolders under pkl_dataset/subjXX/... and keep filename.\n",
    "    parts = mat_path.parts\n",
    "    try:\n",
    "        i = parts.index('osfstorage-archive')\n",
    "    except ValueError:\n",
    "        raise ValueError(f'Path is not under osfstorage-archive: {mat_path}')\n",
    "\n",
    "    subj = parts[i + 1]\n",
    "    if subj not in SUBJECTS:\n",
    "        raise ValueError(f'Not a supported subject folder: {subj} (path: {mat_path})')\n",
    "\n",
    "    rel_under_subj = Path(*parts[i + 2 : -1])  # may be '.'\n",
    "    out_dir = DST_ROOT / subj / rel_under_subj\n",
    "    return out_dir / (mat_path.stem + '.pkl')\n",
    "\n",
    "def lightweight_validate(mat_dict: dict, pkl_dict: dict, mat_path: Path) -> None:\n",
    "    # Keys\n",
    "    mat_keys = sorted(mat_dict.keys())\n",
    "    pkl_keys = sorted(pkl_dict.keys())\n",
    "    if mat_keys != pkl_keys:\n",
    "        raise ValueError(f'Key mismatch for {mat_path}: MAT has {len(mat_keys)} keys, PKL has {len(pkl_keys)} keys')\n",
    "\n",
    "    # Types + basic ndarray structure\n",
    "    for k in mat_keys:\n",
    "        a = mat_dict[k]\n",
    "        b = pkl_dict[k]\n",
    "        if type(a) is not type(b):\n",
    "            raise TypeError(f'Type mismatch for {mat_path} key={k}: MAT={type(a)} PKL={type(b)}')\n",
    "        if isinstance(a, np.ndarray):\n",
    "            if a.shape != b.shape or a.dtype != b.dtype:\n",
    "                raise ValueError(\n",
    "                    f'Array mismatch for {mat_path} key={k}: ' \n",
    "                    f'MAT shape/dtype={a.shape}/{a.dtype} vs PKL={b.shape}/{b.dtype}'\n",
    "                )\n",
    "\n",
    "def convert_one(mat_path: Path) -> ConvertResult:\n",
    "    t0 = time.perf_counter()\n",
    "    pkl_path = dst_path_for(mat_path)\n",
    "    try:\n",
    "        pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load ENTIRE .mat file dict\n",
    "        mat_dict = loadmat(mat_path)\n",
    "\n",
    "        # Write pickle\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(mat_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Lightweight validation (reload pickle, compare keys/types/shape/dtype)\n",
    "        if VALIDATE:\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                pkl_dict = pickle.load(f)\n",
    "            lightweight_validate(mat_dict, pkl_dict, mat_path)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=True, seconds=t1 - t0)\n",
    "    except Exception as e:\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=False, seconds=t1 - t0, error=str(e))\n",
    "\n",
    "# Run conversion with multi-threading\n",
    "results: list[ConvertResult] = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(convert_one, mp): mp for mp in mat_files}\n",
    "    for idx, fut in enumerate(as_completed(futures), start=1):\n",
    "        res = fut.result()\n",
    "        results.append(res)\n",
    "        if idx % 25 == 0 or idx == len(futures):\n",
    "            ok = sum(r.ok for r in results)\n",
    "            print(f'Progress: {idx}/{len(futures)} | ok={ok} | failed={idx-ok}')\n",
    "\n",
    "end = time.perf_counter()\n",
    "print('Done in %.2f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc4a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 2350\n",
      "Succeeded: 2350\n",
      "Failed: 0\n",
      "Mean seconds/file: 0.1337179760463496\n",
      "Median seconds/file: 0.1274758329964243\n",
      "p95 seconds/file: 0.22476303764269684\n"
     ]
    }
   ],
   "source": [
    "total = len(results)\n",
    "ok = sum(r.ok for r in results)\n",
    "failed = total - ok\n",
    "\n",
    "print('Total:', total)\n",
    "print('Succeeded:', ok)\n",
    "print('Failed:', failed)\n",
    "\n",
    "if failed:\n",
    "    print('--- Failures (up to 25) ---')\n",
    "    for r in [x for x in results if not x.ok][:25]:\n",
    "        print('MAT:', r.mat_path)\n",
    "        print('PKL:', r.pkl_path)\n",
    "        print('ERR:', r.error)\n",
    "        print('---')\n",
    "\n",
    "# Basic timing summary (conversion time per file, includes optional validation)\n",
    "times = np.array([r.seconds for r in results], dtype=float) if results else np.array([], dtype=float)\n",
    "if times.size:\n",
    "    print('Mean seconds/file:', float(times.mean()))\n",
    "    print('Median seconds/file:', float(np.median(times)))\n",
    "    print('p95 seconds/file:', float(np.quantile(times, 0.95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d13e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample, decimate\n",
    "import os\n",
    "\n",
    "## Down sample the EMG data and interpolate the glove data\n",
    "## The EMG data is 2048Hz nxt where n=number of channels (134) and t is number of time points\n",
    "## The glove data is 256Hz mxt where m=number of joint angles (18) and t is number of time points\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "PKL_ROOT = ROOT / 'pkl_dataset'\n",
    "OUT_ROOT = ROOT / 'pkl_dataset_resampled'  # new output folder (non-destructive)\n",
    "\n",
    "#### 1: check data in one file to confirm data shape and available vars\n",
    "dirs = [entry.name for entry in os.scandir(PKL_ROOT) if entry.is_dir()]\n",
    "\n",
    "for dir in dirs:\n",
    "    files = [x for x in os.listdir(PKL_ROOT / Path(dir)) if '.pkl' in x]\n",
    "    for file in files:\n",
    "        if 'calibration' in file:\n",
    "            continue\n",
    "        df = pd.read_pickle(PKL_ROOT / Path(dir) / Path(file))\n",
    "        # upsample glove data\n",
    "        glove_fs = 256\n",
    "        resampled_glove_fs = 512\n",
    "        t = np.linspace(0, df['glove'].shape[-1]/glove_fs, df['glove'].shape[-1], endpoint=True)\n",
    "        upsampled_glove_data = resample(df['glove'], num=int(resampled_glove_fs/glove_fs)*df['glove'].shape[-1], t=t, axis=1)[0]\n",
    "\n",
    "        # downsample emg data\n",
    "        emg_fs = 2048\n",
    "        downsampled_emg_fs = 512\n",
    "        downsampled_emg_data = decimate(df['emg'], q=int(emg_fs/downsampled_emg_fs), n=2, axis=1)\n",
    "\n",
    "        df['emg'] = downsampled_emg_data\n",
    "        df['glove'] = upsampled_glove_data\n",
    "\n",
    "        out_dir = OUT_ROOT / dir        # join path with string\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        out_file = out_dir / file\n",
    "\n",
    "        with out_file.open(\"wb\") as f:\n",
    "            pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
