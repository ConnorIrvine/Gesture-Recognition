{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4201e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKL_DATASET_DIR: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/pkl_dataset\n",
      "Created subject subdirs: subj01, subj02, subj03, subj04, subj05, subj06, subj07, subj08, subj09, subj10\n"
     ]
    }
   ],
   "source": [
    "# MAKE THE DIRECTORIES FOR SAVING THE PKL DATASET IF THEY DO NOT EXIST\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "PKL_DATASET_DIR = WORKSPACE / \"pkl_dataset\"\n",
    "SUBJECTS = [f\"subj{n:02d}\" for n in range(1, 11)]\n",
    "\n",
    "PKL_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for subj in SUBJECTS:\n",
    "    (PKL_DATASET_DIR / subj).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PKL_DATASET_DIR:\", PKL_DATASET_DIR)\n",
    "print(\"Created subject subdirs:\", \", \".join(SUBJECTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0d7416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_ROOT: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/osfstorage-archive\n",
      "DST_ROOT: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/pkl_dataset\n",
      "MAX_WORKERS: 8\n",
      "VALIDATE: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "WORKSPACE = Path.cwd()\n",
    "SRC_ROOT = WORKSPACE / 'osfstorage-archive'\n",
    "DST_ROOT = WORKSPACE / 'pkl_dataset'\n",
    "\n",
    "SUBJECTS = [f'subj{n:02d}' for n in range(1, 11)]\n",
    "\n",
    "# Concurrency: tune if you want (I/O + CPU mix).\n",
    "MAX_WORKERS = min(12, (os.cpu_count() or 4))\n",
    "\n",
    "# Validation is an extra load of the pickle (but avoids comparing full arrays).\n",
    "VALIDATE = True\n",
    "\n",
    "print('SRC_ROOT:', SRC_ROOT)\n",
    "print('DST_ROOT:', DST_ROOT)\n",
    "print('MAX_WORKERS:', MAX_WORKERS)\n",
    "print('VALIDATE:', VALIDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb44c2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered .mat files: 2350\n",
      "Example: /Users/aidankirwin/Code/Gesture-Recognition/dataset_cleaning/osfstorage-archive/subj01/detop_exp01_subj01_Sess1_01_01.mat\n"
     ]
    }
   ],
   "source": [
    "def discover_mat_files() -> list[Path]:\n",
    "    mats: list[Path] = []\n",
    "    for subj in SUBJECTS:\n",
    "        subj_dir = SRC_ROOT / subj\n",
    "        if not subj_dir.exists():\n",
    "            raise FileNotFoundError(f'Missing subject directory: {subj_dir}')\n",
    "        mats.extend(sorted(subj_dir.rglob('*.mat')))\n",
    "    return mats\n",
    "\n",
    "mat_files = discover_mat_files()\n",
    "print('Discovered .mat files:', len(mat_files))\n",
    "print('Example:', mat_files[0] if mat_files else 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f20054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 25/2350 | ok=25 | failed=0\n",
      "Progress: 50/2350 | ok=50 | failed=0\n",
      "Progress: 75/2350 | ok=75 | failed=0\n",
      "Progress: 100/2350 | ok=100 | failed=0\n",
      "Progress: 125/2350 | ok=125 | failed=0\n",
      "Progress: 150/2350 | ok=150 | failed=0\n",
      "Progress: 175/2350 | ok=175 | failed=0\n",
      "Progress: 200/2350 | ok=200 | failed=0\n",
      "Progress: 225/2350 | ok=225 | failed=0\n",
      "Progress: 250/2350 | ok=250 | failed=0\n",
      "Progress: 275/2350 | ok=275 | failed=0\n",
      "Progress: 300/2350 | ok=300 | failed=0\n",
      "Progress: 325/2350 | ok=325 | failed=0\n",
      "Progress: 350/2350 | ok=350 | failed=0\n",
      "Progress: 375/2350 | ok=375 | failed=0\n",
      "Progress: 400/2350 | ok=400 | failed=0\n",
      "Progress: 425/2350 | ok=425 | failed=0\n",
      "Progress: 450/2350 | ok=450 | failed=0\n",
      "Progress: 475/2350 | ok=475 | failed=0\n",
      "Progress: 500/2350 | ok=500 | failed=0\n",
      "Progress: 525/2350 | ok=525 | failed=0\n",
      "Progress: 550/2350 | ok=550 | failed=0\n",
      "Progress: 575/2350 | ok=575 | failed=0\n",
      "Progress: 600/2350 | ok=600 | failed=0\n",
      "Progress: 625/2350 | ok=625 | failed=0\n",
      "Progress: 650/2350 | ok=650 | failed=0\n",
      "Progress: 675/2350 | ok=675 | failed=0\n",
      "Progress: 700/2350 | ok=700 | failed=0\n",
      "Progress: 725/2350 | ok=725 | failed=0\n",
      "Progress: 750/2350 | ok=750 | failed=0\n",
      "Progress: 775/2350 | ok=775 | failed=0\n",
      "Progress: 800/2350 | ok=800 | failed=0\n",
      "Progress: 825/2350 | ok=825 | failed=0\n",
      "Progress: 850/2350 | ok=850 | failed=0\n",
      "Progress: 875/2350 | ok=875 | failed=0\n",
      "Progress: 900/2350 | ok=900 | failed=0\n",
      "Progress: 925/2350 | ok=925 | failed=0\n",
      "Progress: 950/2350 | ok=950 | failed=0\n",
      "Progress: 975/2350 | ok=975 | failed=0\n",
      "Progress: 1000/2350 | ok=1000 | failed=0\n",
      "Progress: 1025/2350 | ok=1025 | failed=0\n",
      "Progress: 1050/2350 | ok=1050 | failed=0\n",
      "Progress: 1075/2350 | ok=1075 | failed=0\n",
      "Progress: 1100/2350 | ok=1100 | failed=0\n",
      "Progress: 1125/2350 | ok=1125 | failed=0\n",
      "Progress: 1150/2350 | ok=1150 | failed=0\n",
      "Progress: 1175/2350 | ok=1175 | failed=0\n",
      "Progress: 1200/2350 | ok=1200 | failed=0\n",
      "Progress: 1225/2350 | ok=1225 | failed=0\n",
      "Progress: 1250/2350 | ok=1250 | failed=0\n",
      "Progress: 1275/2350 | ok=1275 | failed=0\n",
      "Progress: 1300/2350 | ok=1300 | failed=0\n",
      "Progress: 1325/2350 | ok=1325 | failed=0\n",
      "Progress: 1350/2350 | ok=1350 | failed=0\n",
      "Progress: 1375/2350 | ok=1375 | failed=0\n",
      "Progress: 1400/2350 | ok=1400 | failed=0\n",
      "Progress: 1425/2350 | ok=1425 | failed=0\n",
      "Progress: 1450/2350 | ok=1450 | failed=0\n",
      "Progress: 1475/2350 | ok=1475 | failed=0\n",
      "Progress: 1500/2350 | ok=1500 | failed=0\n",
      "Progress: 1525/2350 | ok=1525 | failed=0\n",
      "Progress: 1550/2350 | ok=1550 | failed=0\n",
      "Progress: 1575/2350 | ok=1575 | failed=0\n",
      "Progress: 1600/2350 | ok=1600 | failed=0\n",
      "Progress: 1625/2350 | ok=1625 | failed=0\n",
      "Progress: 1650/2350 | ok=1650 | failed=0\n",
      "Progress: 1675/2350 | ok=1675 | failed=0\n",
      "Progress: 1700/2350 | ok=1700 | failed=0\n",
      "Progress: 1725/2350 | ok=1725 | failed=0\n",
      "Progress: 1750/2350 | ok=1750 | failed=0\n",
      "Progress: 1775/2350 | ok=1775 | failed=0\n",
      "Progress: 1800/2350 | ok=1800 | failed=0\n",
      "Progress: 1825/2350 | ok=1825 | failed=0\n",
      "Progress: 1850/2350 | ok=1850 | failed=0\n",
      "Progress: 1875/2350 | ok=1875 | failed=0\n",
      "Progress: 1900/2350 | ok=1900 | failed=0\n",
      "Progress: 1925/2350 | ok=1925 | failed=0\n",
      "Progress: 1950/2350 | ok=1950 | failed=0\n",
      "Progress: 1975/2350 | ok=1975 | failed=0\n",
      "Progress: 2000/2350 | ok=2000 | failed=0\n",
      "Progress: 2025/2350 | ok=2025 | failed=0\n",
      "Progress: 2050/2350 | ok=2050 | failed=0\n",
      "Progress: 2075/2350 | ok=2075 | failed=0\n",
      "Progress: 2100/2350 | ok=2100 | failed=0\n",
      "Progress: 2125/2350 | ok=2125 | failed=0\n",
      "Progress: 2150/2350 | ok=2150 | failed=0\n",
      "Progress: 2175/2350 | ok=2175 | failed=0\n",
      "Progress: 2200/2350 | ok=2200 | failed=0\n",
      "Progress: 2225/2350 | ok=2225 | failed=0\n",
      "Progress: 2250/2350 | ok=2250 | failed=0\n",
      "Progress: 2275/2350 | ok=2275 | failed=0\n",
      "Progress: 2300/2350 | ok=2300 | failed=0\n",
      "Progress: 2325/2350 | ok=2325 | failed=0\n",
      "Progress: 2350/2350 | ok=2350 | failed=0\n",
      "Done in 39.45 seconds\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ConvertResult:\n",
    "    mat_path: Path\n",
    "    pkl_path: Path\n",
    "    ok: bool\n",
    "    seconds: float\n",
    "    error: str | None = None\n",
    "\n",
    "def dst_path_for(mat_path: Path) -> Path:\n",
    "    # Expect mat_path like .../osfstorage-archive/subjXX/<optional subdirs>/file.mat\n",
    "    # Mirror any nested subfolders under pkl_dataset/subjXX/... and keep filename.\n",
    "    parts = mat_path.parts\n",
    "    try:\n",
    "        i = parts.index('osfstorage-archive')\n",
    "    except ValueError:\n",
    "        raise ValueError(f'Path is not under osfstorage-archive: {mat_path}')\n",
    "\n",
    "    subj = parts[i + 1]\n",
    "    if subj not in SUBJECTS:\n",
    "        raise ValueError(f'Not a supported subject folder: {subj} (path: {mat_path})')\n",
    "\n",
    "    rel_under_subj = Path(*parts[i + 2 : -1])  # may be '.'\n",
    "    out_dir = DST_ROOT / subj / rel_under_subj\n",
    "    return out_dir / (mat_path.stem + '.pkl')\n",
    "\n",
    "def lightweight_validate(mat_dict: dict, pkl_dict: dict, mat_path: Path) -> None:\n",
    "    # Keys\n",
    "    mat_keys = sorted(mat_dict.keys())\n",
    "    pkl_keys = sorted(pkl_dict.keys())\n",
    "    if mat_keys != pkl_keys:\n",
    "        raise ValueError(f'Key mismatch for {mat_path}: MAT has {len(mat_keys)} keys, PKL has {len(pkl_keys)} keys')\n",
    "\n",
    "    # Types + basic ndarray structure\n",
    "    for k in mat_keys:\n",
    "        a = mat_dict[k]\n",
    "        b = pkl_dict[k]\n",
    "        if type(a) is not type(b):\n",
    "            raise TypeError(f'Type mismatch for {mat_path} key={k}: MAT={type(a)} PKL={type(b)}')\n",
    "        if isinstance(a, np.ndarray):\n",
    "            if a.shape != b.shape or a.dtype != b.dtype:\n",
    "                raise ValueError(\n",
    "                    f'Array mismatch for {mat_path} key={k}: ' \n",
    "                    f'MAT shape/dtype={a.shape}/{a.dtype} vs PKL={b.shape}/{b.dtype}'\n",
    "                )\n",
    "\n",
    "def convert_one(mat_path: Path) -> ConvertResult:\n",
    "    t0 = time.perf_counter()\n",
    "    pkl_path = dst_path_for(mat_path)\n",
    "    try:\n",
    "        pkl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load ENTIRE .mat file dict\n",
    "        mat_dict = loadmat(mat_path)\n",
    "\n",
    "        # Write pickle\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pickle.dump(mat_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Lightweight validation (reload pickle, compare keys/types/shape/dtype)\n",
    "        if VALIDATE:\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                pkl_dict = pickle.load(f)\n",
    "            lightweight_validate(mat_dict, pkl_dict, mat_path)\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=True, seconds=t1 - t0)\n",
    "    except Exception as e:\n",
    "        t1 = time.perf_counter()\n",
    "        return ConvertResult(mat_path=mat_path, pkl_path=pkl_path, ok=False, seconds=t1 - t0, error=str(e))\n",
    "\n",
    "# Run conversion with multi-threading\n",
    "results: list[ConvertResult] = []\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {ex.submit(convert_one, mp): mp for mp in mat_files}\n",
    "    for idx, fut in enumerate(as_completed(futures), start=1):\n",
    "        res = fut.result()\n",
    "        results.append(res)\n",
    "        if idx % 25 == 0 or idx == len(futures):\n",
    "            ok = sum(r.ok for r in results)\n",
    "            print(f'Progress: {idx}/{len(futures)} | ok={ok} | failed={idx-ok}')\n",
    "\n",
    "end = time.perf_counter()\n",
    "print('Done in %.2f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc4a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 2350\n",
      "Succeeded: 2350\n",
      "Failed: 0\n",
      "Mean seconds/file: 0.1337179760463496\n",
      "Median seconds/file: 0.1274758329964243\n",
      "p95 seconds/file: 0.22476303764269684\n"
     ]
    }
   ],
   "source": [
    "total = len(results)\n",
    "ok = sum(r.ok for r in results)\n",
    "failed = total - ok\n",
    "\n",
    "print('Total:', total)\n",
    "print('Succeeded:', ok)\n",
    "print('Failed:', failed)\n",
    "\n",
    "if failed:\n",
    "    print('--- Failures (up to 25) ---')\n",
    "    for r in [x for x in results if not x.ok][:25]:\n",
    "        print('MAT:', r.mat_path)\n",
    "        print('PKL:', r.pkl_path)\n",
    "        print('ERR:', r.error)\n",
    "        print('---')\n",
    "\n",
    "# Basic timing summary (conversion time per file, includes optional validation)\n",
    "times = np.array([r.seconds for r in results], dtype=float) if results else np.array([], dtype=float)\n",
    "if times.size:\n",
    "    print('Mean seconds/file:', float(times.mean()))\n",
    "    print('Median seconds/file:', float(np.median(times)))\n",
    "    print('p95 seconds/file:', float(np.quantile(times, 0.95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample, decimate\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Down sample the EMG data and interpolate the glove data\n",
    "- The EMG data is 2048Hz nxt where n=number of channels (134) and t is number of time points\n",
    "- The glove data is 256Hz mxt where m=number of joint angles (18) and t is number of time points\n",
    "\"\"\"\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "PKL_ROOT = ROOT / 'pkl_dataset'\n",
    "OUT_ROOT = ROOT / 'pkl_dataset_resampled'  # new output folder (non-destructive)\n",
    "GLOVE_FS = 256\n",
    "EMG_FS = 2048\n",
    "DOWNSAMPLED_EMG_FS = 512\n",
    "RESAMPLED_GLOVE_FS = DOWNSAMPLED_EMG_FS\n",
    "\n",
    "#### 1: check data in one file to confirm data shape and available vars\n",
    "dirs = [entry.name for entry in os.scandir(PKL_ROOT) if entry.is_dir()]\n",
    "\n",
    "for dir in dirs:\n",
    "    files = [x for x in os.listdir(PKL_ROOT / Path(dir)) if '.pkl' in x]\n",
    "    for file in files:\n",
    "        if 'calibration' in file:\n",
    "            continue\n",
    "        df = pd.read_pickle(PKL_ROOT / Path(dir) / Path(file))\n",
    "        # upsample glove data\n",
    "        t = np.linspace(0, df['glove'].shape[-1]/GLOVE_FS, df['glove'].shape[-1], endpoint=True)\n",
    "        upsampled_glove_data = resample(df['glove'], num=int(RESAMPLED_GLOVE_FS/GLOVE_FS)*df['glove'].shape[-1], t=t, axis=1)[0]\n",
    "\n",
    "        # downsample emg data\n",
    "        downsampled_emg_data = decimate(df['emg'], q=int(EMG_FS/DOWNSAMPLED_EMG_FS), n=2, axis=1)\n",
    "\n",
    "        df['emg'] = downsampled_emg_data\n",
    "        df['glove'] = upsampled_glove_data\n",
    "\n",
    "        out_dir = OUT_ROOT / dir        # join path with string\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        out_file = out_dir / file\n",
    "\n",
    "        with out_file.open(\"wb\") as f:\n",
    "            pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "df153b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features (from SEED dataset)\n",
    "\"\"\"\n",
    "\n",
    "def MAV(data):\n",
    "    \"\"\"\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: MAV of the data\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(data), axis=1).reshape((1, data.shape[0]))\n",
    "\n",
    "def VAR(data):\n",
    "    \"\"\"\n",
    "    Variance\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: Variance of the data\n",
    "    \"\"\"\n",
    "    return np.var(data, axis=1).reshape((1, data.shape[0]))\n",
    "\n",
    "def RMS(data):\n",
    "    \"\"\"\n",
    "    Root mean square\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: RMS per channel\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(data**2, axis=1)).reshape((1, data.shape[0]))\n",
    "\n",
    "def zero_crossings(data):\n",
    "    \"\"\"\n",
    "    Number of zero crossings from each channel\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: number of zero crossings for each channel\n",
    "    \"\"\"\n",
    "    positive = data > 0\n",
    "    return np.sum(np.bitwise_xor(positive[:, 1:], positive[:, :-1]), axis=1).reshape((1, data.shape[0]))\n",
    "\n",
    "def avg_amplitude_change(data):\n",
    "    \"\"\"\n",
    "    https://www.sciencedirect.com/science/article/pii/S0957417412001200\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: Average amplitude change (AAC) as defined in the reference above\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(np.diff(data, axis=1)), axis=1).reshape((1, data.shape[0]))\n",
    "\n",
    "def spectral_centroid(data):\n",
    "    \"\"\"\n",
    "    Spectral Centroid\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: Spectral centroid per channel (FFT-bin based)\n",
    "    \"\"\"\n",
    "    # Magnitude spectrum\n",
    "    spectrum = np.abs(np.fft.rfft(data, axis=1))\n",
    "    \n",
    "    # Frequency bin indices\n",
    "    freqs = np.arange(spectrum.shape[1])\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    denom = np.sum(spectrum, axis=1) + 1e-12\n",
    "    \n",
    "    centroid = np.sum(spectrum * freqs, axis=1) / denom\n",
    "    return centroid.reshape((1, data.shape[0]))\n",
    "\n",
    "\n",
    "def spectral_spread(data):\n",
    "    \"\"\"\n",
    "    Spectral Spread\n",
    "    :param data: 2D array, channels x samples\n",
    "    :return: Spectral spread per channel (FFT-bin based)\n",
    "    \"\"\"\n",
    "    # Magnitude spectrum\n",
    "    spectrum = np.abs(np.fft.rfft(data, axis=1))\n",
    "    \n",
    "    # Frequency bin indices\n",
    "    freqs = np.arange(spectrum.shape[1])\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    denom = np.sum(spectrum, axis=1) + 1e-12\n",
    "    \n",
    "    # Spectral centroid\n",
    "    centroid = np.sum(spectrum * freqs, axis=1) / denom\n",
    "    \n",
    "    # Spectral spread (standard deviation around centroid)\n",
    "    spread = np.sqrt(\n",
    "        np.sum(spectrum * (freqs - centroid[:, None])**2, axis=1) / denom\n",
    "    )\n",
    "    \n",
    "    return spread.reshape((1, data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4a40eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find labels for file detop_exp01_subj02_Sess3_02_02.pkl\n",
      "Movement: ['index_flex'] | Speed: ['fast']\n",
      "Start time was less than end time for file detop_exp01_subj03_Sess3_07_04.pkl\n",
      "Start time: 0.638671875 | End time: 0.0\n",
      "Continuing to next file\n",
      "Could not find labels for file detop_exp01_subj04_Sess2_11_02.pkl\n",
      "Movement: ['pinch'] | Speed: ['slow']\n",
      "Could not find labels for file detop_exp01_subj04_Sess1_03_05.pkl\n",
      "Movement: ['point'] | Speed: ['slow']\n",
      "Could not find labels for file detop_exp01_subj04_Sess3_02_03.pkl\n",
      "Movement: ['index'] | Speed: ['fast']\n",
      "Could not find labels for file detop_exp01_subj04_Sess3_02_02.pkl\n",
      "Movement: ['index'] | Speed: ['slow']\n",
      "Could not find labels for file detop_exp01_subj04_Sess1_05_06.pkl\n",
      "Movement: ['3digit'] | Speed: ['fast']\n",
      "Start time was less than end time for file detop_exp01_subj04_Sess1_05_01.pkl\n",
      "Start time: 1.8515625 | End time: 0.6796875\n",
      "Continuing to next file\n",
      "Start time was less than end time for file detop_exp01_subj10_Sess1_13_04.pkl\n",
      "Start time: 0.58984375 | End time: 0.0\n",
      "Continuing to next file\n",
      "Start time was less than end time for file detop_exp01_subj08_Sess2_08_05.pkl\n",
      "Start time: 4.17578125 | End time: 0.296875\n",
      "Continuing to next file\n",
      "Could not find labels for file detop_exp01_subj01_Sess2_13_01.pkl\n",
      "Movement: ['point'] | Speed: ['fast']\n",
      "Could not find labels for file detop_exp01_subj01_Sess1_12_06.pkl\n",
      "Movement: ['index_flex'] | Speed: ['fast']\n",
      "Could not find labels for file detop_exp01_subj01_Sess1_12_03.pkl\n",
      "Movement: ['index_flex'] | Speed: ['slow']\n",
      "Start time was less than end time for file detop_exp01_subj07_Sess3_02_01.pkl\n",
      "Start time: 3.29296875 | End time: 0.4375\n",
      "Continuing to next file\n",
      "Start time was less than end time for file detop_exp01_subj07_Sess3_05_01.pkl\n",
      "Start time: 5.30078125 | End time: 0.3125\n",
      "Continuing to next file\n",
      "Start time was less than end time for file detop_exp01_subj07_Sess3_05_04.pkl\n",
      "Start time: 5.267578125 | End time: 0.482421875\n",
      "Continuing to next file\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "\"\"\"\n",
    "Create a dataset for classification training/testing.\n",
    "Each participant is stored in a separate file.\n",
    "\"\"\"\n",
    "\n",
    "def extract_windows(data, num_windows, window_size, window_step):\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        start = int(i*window_step)\n",
    "        end = int(start + window_size)\n",
    "        win_data = data[:, start:end]\n",
    "        windows.append(win_data)\n",
    "    return windows\n",
    "\n",
    "def num_windows(length, window_size, window_step):\n",
    "    return int(math.floor((length - window_size) / window_step)) + 1\n",
    "\n",
    "def extract_features_by_window(data, num_windows, window_size, window_step):\n",
    "    \"\"\"\n",
    "    Splits data set into windows, extracts features for each window\n",
    "    \"\"\"\n",
    "\n",
    "    feature_names = ['mav', 'var', 'rms', 'zcr', 'aac', 'sc', 'ss']\n",
    "\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        start = int(i*window_step)\n",
    "        end = int(start + window_size)\n",
    "        win_data = data[:, start:end]\n",
    "\n",
    "        feature_array = []\n",
    "        feature_array.append(MAV(win_data))\n",
    "        feature_array.append(VAR(win_data))\n",
    "        feature_array.append(RMS(win_data))\n",
    "        feature_array.append(zero_crossings(win_data))\n",
    "        feature_array.append(avg_amplitude_change(win_data))\n",
    "        feature_array.append(spectral_centroid(win_data))\n",
    "        feature_array.append(spectral_spread(win_data))\n",
    "        \n",
    "        windows.append(win_data)\n",
    "    return windows, feature_names\n",
    "\n",
    "\n",
    "# file handling\n",
    "ROOT = Path.cwd()\n",
    "LABELS = ROOT / 'labels'\n",
    "DATA = ROOT / 'pkl_dataset_resampled'\n",
    "\n",
    "# windowing\n",
    "FS = 512\n",
    "WINDOW_SIZE = int(0.2*FS)   # 200 ms windows\n",
    "WINDOW_HOP = WINDOW_SIZE // 2 # 50% overlap (NOTE: WINDOW_HOP = WINDOW_SIZE - WINDOW_OVERLAP)\n",
    "\n",
    "## Load labels data\n",
    "labels = pd.read_csv(LABELS / 'labels.csv', index_col=False)\n",
    "kill_list = pd.read_csv(LABELS / 'kill_list.csv', index_col=False)\n",
    "kill_list = kill_list['file'].to_list()\n",
    "\n",
    "## Create empty data frame for all data\n",
    "dataset_columns = {'movement': [], 'speed': [], 'subject': [], 'session': [], 'trial': [], 'windows_raw': [], 'windows_feature': []}\n",
    "df = pd.DataFrame(dataset_columns)\n",
    "\n",
    "## Load in dataset\n",
    "dirs = [entry.name for entry in os.scandir(DATA) if entry.is_dir()]\n",
    "for dir in dirs:\n",
    "    files = [x for x in os.listdir(DATA / dir) if '.pkl' in x]\n",
    "    for file in files:\n",
    "        # load data file\n",
    "        temp = pd.read_pickle(DATA / dir / file)\n",
    "\n",
    "        if temp['movement'] == 'disc' or temp['movement'] == 'thumbAdd' or file in kill_list:\n",
    "            continue\n",
    "\n",
    "        temp_row = dataset_columns.copy()\n",
    "\n",
    "        # get labels\n",
    "        file_labs = labels.loc[labels['file'] == file].to_dict(orient='records') # note if this returns more than 1 row something is wrong\n",
    "        # print(file_labs)\n",
    "        try:\n",
    "            file_labs = file_labs[0]\n",
    "            try:\n",
    "                start_sample = int(file_labs['start'] * FS)\n",
    "                end_sample = int(file_labs['end'] * FS)\n",
    "                # print(f\"Start time: {start_sample / FS} | End time: {end_sample / FS}\")\n",
    "            except:\n",
    "                raise ValueError(f'Could not access start')\n",
    "        except:\n",
    "                print(f\"Could not find labels for file {file}\")\n",
    "                print(f\"Movement: {temp['movement']} | Speed: {temp['speed']}\")\n",
    "        \n",
    "        if end_sample < start_sample:\n",
    "            print(f\"Start time was less than end time for file {file}\")\n",
    "            print(f\"Start time: {start_sample / FS} | End time: {end_sample / FS}\")\n",
    "            print(\"Continuing to next file\")\n",
    "            continue\n",
    "\n",
    "        emg = temp['emg']\n",
    "        emg = emg[start_sample:end_sample]\n",
    "\n",
    "        # extract windows\n",
    "        n_windows = num_windows(emg.shape[1], WINDOW_SIZE, WINDOW_HOP)\n",
    "        windows = extract_windows(emg, n_windows, WINDOW_SIZE, WINDOW_HOP)\n",
    "        feature_windows = extract_features_by_window(emg, n_windows, WINDOW_SIZE, WINDOW_HOP)\n",
    "        ## NOTE: we will need glove data later for regression, leave out for now\n",
    "        # glove = temp['glove']\n",
    "\n",
    "        ## append data to dataframe\n",
    "        temp_row['windows_raw'] = windows\n",
    "        temp_row['windows_features'] = feature_windows\n",
    "        temp_row['speed'] = str(temp['speed'][0])\n",
    "        temp_row['subject'] = str(temp['subject'][0])\n",
    "        temp_row['movement'] = str(temp['movement'][0])\n",
    "        temp_row['session'] = int(temp['session'][0][0])\n",
    "        temp_row['trial'] = int(str(file).split('.')[0].split('_')[-1])\n",
    "\n",
    "        df_the_dict = pd.DataFrame([temp_row])\n",
    "        df = pd.concat([df, df_the_dict], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb88d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592\n",
      "1273\n",
      "319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = df\n",
    "\n",
    "### Test training split\n",
    "print(len(dataset))\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['windows_feature'], dataset['movement'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "### Train linear SVM, classify fist vs pinch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
